{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9caa4968-8087-4a4d-acc5-1a5594af6c9e",
   "metadata": {},
   "source": [
    "## FOLDER RESTRUCTURING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a1732a-e2ac-4bee-a1ac-6f2461a30819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found categories: ['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
      "\n",
      "Copy summary:\n",
      "  bottle       -> train: 209, val:  20\n",
      "  cable        -> train: 224, val:  58\n",
      "  capsule      -> train: 219, val:  23\n",
      "  carpet       -> train: 280, val:  28\n",
      "  grid         -> train: 264, val:  21\n",
      "  hazelnut     -> train: 391, val:  40\n",
      "  leather      -> train: 245, val:  32\n",
      "  metal_nut    -> train: 220, val:  22\n",
      "  pill         -> train: 267, val:  26\n",
      "  screw        -> train: 320, val:  41\n",
      "  tile         -> train: 230, val:  33\n",
      "  toothbrush   -> train:  60, val:  12\n",
      "  transistor   -> train: 213, val:  60\n",
      "  wood         -> train: 247, val:  19\n",
      "  zipper       -> train: 240, val:  32\n",
      "\n",
      "Total images copied: train=3629, val=467\n",
      "New structure ready at: mvtec_all/ (use ImageFolder on mvtec_all/train and mvtec_all/val)\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# from pathlib import Path\n",
    "# import random\n",
    "\n",
    "# SOURCE_ROOT = \"/Users/mrinalseth13331/Downloads/archive\"         # your original dataset root\n",
    "# TARGET_ROOT = \"mvtec_all\"     # new folder that ImageFolder expects\n",
    "# VAL_LIMIT_PER_CLASS = None    # None -> copy all; or set int (e.g., 50) to limit validation images\n",
    "# SHUFFLE_VAL = True            # if True, pick random images from test/good for val (useful when many)\n",
    "# OVERWRITE = False             # if True, overwrite existing files in target\n",
    "\n",
    "\n",
    "# SOURCE_ROOT = Path(SOURCE_ROOT)\n",
    "# TARGET_ROOT = Path(TARGET_ROOT)\n",
    "\n",
    "# if not SOURCE_ROOT.exists():\n",
    "#     raise FileNotFoundError(f\"Source root {SOURCE_ROOT} not found\")\n",
    "\n",
    "# # create train/val root\n",
    "# train_root = TARGET_ROOT / \"train\"\n",
    "# val_root = TARGET_ROOT / \"val\"\n",
    "# train_root.mkdir(parents=True, exist_ok=True)\n",
    "# val_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# categories = sorted([p.name for p in SOURCE_ROOT.iterdir() if p.is_dir()])\n",
    "# print(\"Found categories:\", categories)\n",
    "\n",
    "# summary = {}\n",
    "\n",
    "# for cat in categories:\n",
    "#     src_train_good = SOURCE_ROOT / cat / \"train\" / \"good\"\n",
    "#     src_test_good = SOURCE_ROOT / cat / \"test\" / \"good\"\n",
    "\n",
    "#     dst_train_cat = train_root / cat\n",
    "#     dst_val_cat = val_root / cat\n",
    "#     dst_train_cat.mkdir(parents=True, exist_ok=True)\n",
    "#     dst_val_cat.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # copy train/good -> mvtec_all/train/<cat>/\n",
    "#     train_files = []\n",
    "#     if src_train_good.exists():\n",
    "#         train_files = sorted([f for f in src_train_good.iterdir() if f.is_file()])\n",
    "#         for f in train_files:\n",
    "#             dst = dst_train_cat / f.name\n",
    "#             if dst.exists() and not OVERWRITE:\n",
    "#                 continue\n",
    "#             shutil.copy2(f, dst)\n",
    "#     else:\n",
    "#         print(f\"Warning: {src_train_good} does not exist for category {cat}\")\n",
    "\n",
    "#     # prepare val files from test/good\n",
    "#     val_files = []\n",
    "#     if src_test_good.exists():\n",
    "#         val_candidates = sorted([f for f in src_test_good.iterdir() if f.is_file()])\n",
    "#         if SHUFFLE_VAL:\n",
    "#             random.shuffle(val_candidates)\n",
    "#         if VAL_LIMIT_PER_CLASS is not None:\n",
    "#             val_candidates = val_candidates[:VAL_LIMIT_PER_CLASS]\n",
    "#         for f in val_candidates:\n",
    "#             dst = dst_val_cat / f.name\n",
    "#             if dst.exists() and not OVERWRITE:\n",
    "#                 continue\n",
    "#             shutil.copy2(f, dst)\n",
    "#         val_files = val_candidates\n",
    "#     else:\n",
    "#         # If test/good missing, optionally use some from train/good for val (fallback)\n",
    "#         if train_files:\n",
    "#             fallback = train_files[:min(10, len(train_files))]  # small fallback\n",
    "#             for f in fallback:\n",
    "#                 dst = dst_val_cat / f.name\n",
    "#                 if dst.exists() and not OVERWRITE:\n",
    "#                     continue\n",
    "#                 shutil.copy2(f, dst)\n",
    "#             val_files = fallback\n",
    "#         else:\n",
    "#             print(f\"Warning: No test/good or train/good found for {cat}\")\n",
    "\n",
    "#     summary[cat] = {\n",
    "#         \"train_copied\": len(list(dst_train_cat.iterdir())),\n",
    "#         \"val_copied\": len(list(dst_val_cat.iterdir()))\n",
    "#     }\n",
    "\n",
    "# # Print summary\n",
    "# print(\"\\nCopy summary:\")\n",
    "# total_train = total_val = 0\n",
    "# for cat, info in summary.items():\n",
    "#     print(f\"  {cat:12s} -> train: {info['train_copied']:3d}, val: {info['val_copied']:3d}\")\n",
    "#     total_train += info['train_copied']\n",
    "#     total_val += info['val_copied']\n",
    "\n",
    "# print(f\"\\nTotal images copied: train={total_train}, val={total_val}\")\n",
    "# print(f\"New structure ready at: {TARGET_ROOT}/ (use ImageFolder on {TARGET_ROOT}/train and {TARGET_ROOT}/val)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cd736cf-6a2a-400c-95d2-62f979d6c008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (0.24.0)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.13/site-packages (11.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.13/site-packages (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (4.7.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.122.0)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (2.0.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (1.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (2.12.4)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.13/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/mrinalseth13331/.local/lib/python3.13/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.0)\n",
      "Requirement already satisfied: typer-slim in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.20.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision pillow tqdm matplotlib scikit-learn gradio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b6bfed-27c5-4c6f-a1f6-5050d2859a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84817003-7292-43d4-af81-e0fa7171eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class ResNetFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True, device=None):\n",
    "        super().__init__()\n",
    "        weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
    "        self.model = resnet50(weights=weights)\n",
    "        self.model.eval()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.features = []\n",
    "        self.handles = []\n",
    "        self.handles.append(self.model.layer2[-1].register_forward_hook(self._hook))\n",
    "        self.handles.append(self.model.layer3[-1].register_forward_hook(self._hook))\n",
    "\n",
    "        if device is not None:\n",
    "            self.to(device)\n",
    "\n",
    "    def _hook(self, module, input, output):\n",
    "        # detach and keep device (no cpu() here)\n",
    "        self.features.append(output.detach())\n",
    "\n",
    "    def forward(self, x, target_spatial=None):\n",
    "        self.features = []\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(x)\n",
    "\n",
    "        if len(self.features) == 0:\n",
    "            raise RuntimeError(\"No features captured by hooks\")\n",
    "\n",
    "        # default target: first fmap size\n",
    "        if target_spatial is None:\n",
    "            h, w = self.features[0].shape[-2], self.features[0].shape[-1]\n",
    "            target_spatial = (h, w)\n",
    "\n",
    "        resized = []\n",
    "        for fmap in self.features:\n",
    "            fmap_smoothed = F.avg_pool2d(fmap, kernel_size=3, stride=1, padding=1)  # keep spatial dims\n",
    "            if fmap_smoothed.shape[-2:] != target_spatial:\n",
    "                fmap_resized = F.interpolate(fmap_smoothed, size=target_spatial, mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                fmap_resized = fmap_smoothed\n",
    "            resized.append(fmap_resized)\n",
    "\n",
    "        patch = torch.cat(resized, dim=1)  # (B, 512+1024=1536, H, W)\n",
    "        return patch\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for h in self.handles:\n",
    "            h.remove()\n",
    "        self.handles = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7b31b1-e989-4f34-8f35-5c92fc1264e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ResNetFeatureExtractor(pretrained=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e447ef1-9c11-44f8-b7a2-87d5aafd5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = \"/Users/mrinalseth13331/Downloads/archive\"  # path where all class folders exist\n",
    "\n",
    "category_list = sorted([d for d in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, d))])\n",
    "\n",
    "print(category_list)\n",
    "print(len(category_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28913941-2afa-4a12-ba46-020533a66d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "num_classes = len(category_list)  # e.g., 15\n",
    "\n",
    "clf = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# replace final layer\n",
    "clf.fc = torch.nn.Linear(clf.fc.in_features, num_classes)\n",
    "clf = clf.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfc960a-7117-40e9-9dfc-f96fc5907f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN,\n",
    "                         std=IMAGENET_STD)\n",
    "])\n",
    "train_ds = ImageFolder(\"mvtec_all/train\", transform=transform)  # folder containing class-subfolders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = ImageFolder(\"mvtec_all/val\", transform=transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ccb12c-f7fe-4890-86c6-459427016b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(clf.parameters(), lr=1e-4)\n",
    "\n",
    "def train_classifier(model, train_loader, val_loader=None, epochs=5, device=device, checkpoint_path=\"best_classifier.pth\"):\n",
    "    \"\"\"\n",
    "    Train a classifier (assumes optimizer & criterion already defined in the outer scope).\n",
    "    Returns: history dict with train_loss, val_loss, train_acc, val_acc lists.\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optional scheduler example (uncomment if you like)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [train]\", leave=False)\n",
    "        for imgs, labels in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)                 # logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # stats\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "\n",
    "            pbar.set_postfix({\"batch_loss\": loss.item(), \"acc\": running_corrects/total})\n",
    "\n",
    "        epoch_train_loss = running_loss / max(1, total)\n",
    "        epoch_train_acc = running_corrects / max(1, total)\n",
    "        history[\"train_loss\"].append(epoch_train_loss)\n",
    "        history[\"train_acc\"].append(epoch_train_acc)\n",
    "\n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_corrects = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [val]\", leave=False):\n",
    "                    imgs = imgs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item() * imgs.size(0)\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    val_corrects += (preds == labels).sum().item()\n",
    "                    val_total += imgs.size(0)\n",
    "\n",
    "            epoch_val_loss = val_loss / max(1, val_total)\n",
    "            epoch_val_acc = val_corrects / max(1, val_total)\n",
    "            history[\"val_loss\"].append(epoch_val_loss)\n",
    "            history[\"val_acc\"].append(epoch_val_acc)\n",
    "\n",
    "\n",
    "            # checkpoint best\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"val_loss\": epoch_val_loss\n",
    "                }, checkpoint_path)\n",
    "\n",
    "            print(f\"Epoch {epoch}/{epochs}  Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}  |  Val loss: {epoch_val_loss:.4f}, Val acc: {epoch_val_acc:.4f}\")\n",
    "        else:\n",
    "            # no validation loader provided\n",
    "            history[\"val_loss\"].append(None)\n",
    "            history[\"val_acc\"].append(None)\n",
    "            print(f\"Epoch {epoch}/{epochs}  Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "204c8479-1f2c-4b2f-9c07-6a0763fbbc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903782c81a6c432a94d29deb12086194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [train]:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883581d4faf402dbe53c23a116b34be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [val]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Train loss: 0.0992, Train acc: 0.9829  |  Val loss: 0.0009, Val acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544375c5d2de489c82edd9a73dafc5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [train]:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4127b2960d40ee885090e5b859e19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [val]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Train loss: 0.0032, Train acc: 1.0000  |  Val loss: 0.0005, Val acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f83d5a12dca434b81767d25f8c5673a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [train]:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f5fa6ff89c44629e4d48de14c7fda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [val]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Train loss: 0.0013, Train acc: 1.0000  |  Val loss: 0.0003, Val acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2984dd37494cd7b5fc129af0c6c927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [train]:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2178eac75ac54bf1b1592ae8cffe8320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [val]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Train loss: 0.0009, Train acc: 1.0000  |  Val loss: 0.0002, Val acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567cbb575e33420697531d9396c2bc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [train]:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee20ba3cbf234247a2fa40fba701d410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [val]:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Train loss: 0.0006, Train acc: 1.0000  |  Val loss: 0.0001, Val acc: 1.0000\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "history = train_classifier(\n",
    "    model=clf,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=5,                     # change as needed\n",
    "    device=device,\n",
    "    checkpoint_path=\"best_classifier.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a431e73-8e88-465d-a3dd-88a905308546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "clf.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = clf(imgs)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(\"Validation Accuracy: \", correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd3df7-902a-4b28-8d2c-f2ba08eb612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clf.state_dict(), \"classifier_res18.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1fc788-9910-4a46-b316-d8290fda9dde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m ImageFolder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmvtec_all/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      2\u001b[0m train_ds\u001b[38;5;241m.\u001b[39mclasses           \u001b[38;5;66;03m# list of class names\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train_ds\u001b[38;5;241m.\u001b[39mclass_to_idx      \u001b[38;5;66;03m# dictionary mapping names -> index\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageFolder' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = ImageFolder(\"mvtec_all/train\", transform=transform)\n",
    "train_ds.classes           # list of class names\n",
    "train_ds.class_to_idx      # dictionary mapping names -> index\n",
    "print(train_ds.classes)\n",
    "category_list = train_ds.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "280d748b-f57f-4e59-8f41-6b9f9aa49863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Get class names from ImageFolder\n",
    "category_list = train_ds.classes\n",
    "print(\"Class names:\", category_list)\n",
    "\n",
    "def classify_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = clf(x)\n",
    "        pred_idx = logits.argmax(1).item()\n",
    "\n",
    "    pred_name = category_list[pred_idx]\n",
    "    print(\"Predicted object:\", pred_name)\n",
    "\n",
    "    return pred_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc6a35ef-69a0-4c6b-8d7c-7e783abcbbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted object: zipper\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'zipper'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_image('/Users/mrinalseth13331/Downloads/mvtec_all/train/zipper/238.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b1fc4-999a-46a5-a9d8-0b3b1f8223f2",
   "metadata": {},
   "source": [
    "#### In this taken help from the generative pre-trained transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df66ccea-401a-4f3c-aab2-005c870732b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No `clf` in memory and no `train_ds` found to infer classes. Either define `clf` or place train_ds in the notebook or provide 'best_classifier.pth' and category_list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(category_list)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `clf` in memory and no `train_ds` found to infer classes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither define `clf` or place train_ds in the notebook or provide \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_classifier.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and category_list.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# rebuild architecture and load\u001b[39;00m\n\u001b[1;32m     41\u001b[0m clf \u001b[38;5;241m=\u001b[39m resnet18(weights\u001b[38;5;241m=\u001b[39mResNet18_Weights\u001b[38;5;241m.\u001b[39mDEFAULT)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No `clf` in memory and no `train_ds` found to infer classes. Either define `clf` or place train_ds in the notebook or provide 'best_classifier.pth' and category_list."
     ]
    }
   ],
   "source": [
    "# Gradio app: predict only the object name\n",
    "import torch\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "# -------- device --------\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------- transform (ImageNet normalization) --------\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# -------- classifier: use existing `clf` if present, else try to load checkpoint --------\n",
    "try:\n",
    "    clf  # if clf exists in the notebook, use it\n",
    "    print(\"Using existing classifier 'clf' from notebook.\")\n",
    "except NameError:\n",
    "    clf = None\n",
    "\n",
    "if clf is None:\n",
    "    ckpt_path = \"best_classifier.pth\"\n",
    "    if os.path.exists(ckpt_path):\n",
    "        from torchvision.models import resnet18, ResNet18_Weights\n",
    "        # we need category_list to know num_classes; try to infer from train_ds if present\n",
    "        if 'train_ds' in globals():\n",
    "            category_list = train_ds.classes\n",
    "            num_classes = len(category_list)\n",
    "        else:\n",
    "            raise RuntimeError(\"No `clf` in memory and no `train_ds` found to infer classes. \"\n",
    "                               \"Either define `clf` or place train_ds in the notebook or provide 'best_classifier.pth' and category_list.\")\n",
    "        # rebuild architecture and load\n",
    "        clf = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        clf.fc = torch.nn.Linear(clf.fc.in_features, num_classes)\n",
    "        state = torch.load(ckpt_path, map_location=device)\n",
    "        clf.load_state_dict(state[\"model_state_dict\"])\n",
    "        print(f\"Loaded classifier from {ckpt_path}.\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No classifier `clf` found and no checkpoint at 'best_classifier.pth'.\")\n",
    "\n",
    "# move to device and eval\n",
    "clf = clf.to(device)\n",
    "clf.eval()\n",
    "\n",
    "if 'train_ds' in globals():\n",
    "    category_list = train_ds.classes\n",
    "elif 'category_list' in globals():\n",
    "    pass\n",
    "else:\n",
    "    raise RuntimeError(\"Cannot find `category_list`. Ensure `train_ds` exists or define `category_list` list in the notebook.\")\n",
    "\n",
    "# -------- prediction function for Gradio --------\n",
    "def predict_object_name(pil_img: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    pil_img: PIL.Image from Gradio\n",
    "    returns: predicted class name (string)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = pil_img.convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(device)          # (1,3,224,224)\n",
    "        with torch.no_grad():\n",
    "            logits = clf(x)\n",
    "            pred_idx = int(logits.argmax(dim=1).item())\n",
    "        pred_name = category_list[pred_idx]\n",
    "        return str(pred_name)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# -------- Gradio interface --------\n",
    "iface = gr.Interface(\n",
    "    fn=predict_object_name,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload image\"),\n",
    "    outputs=gr.Textbox(label=\"Predicted object\"),\n",
    "    title=\"MVTec Object Classifier (name only)\",\n",
    "    description=\"Upload an image; the model will predict the object category name (e.g., 'bottle').\"\n",
    ")\n",
    "\n",
    "# Launch\n",
    "iface.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcfc3a-9d8b-4e78-9c22-00a71f18369a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
