{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23c6fa7-3f17-4886-a732-b77e27bb9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune these hyperparams as needed\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "LATENT_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052e318f-7a1c-4784-9987-9a26eb0a0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeatCAE(nn.Module):\n",
    "    \"\"\"Autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=1000, latent_dim=50, is_bn=True):\n",
    "        super(FeatCAE, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(in_channels, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d(2 * latent_dim, latent_dim, kernel_size=1, stride=1, padding=0)]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        # if 1x1 conv to reconstruct the rgb values, we try to learn a linear combination\n",
    "        # of the features for rgb\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(latent_dim, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d(2 * latent_dim, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, in_channels, kernel_size=1, stride=1, padding=0)]\n",
    "        # layers += [nn.ReLU()]\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca63498-475b-4913-a06c-46de01a7b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class ResNetFeatureExtractorFast(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True, device=None):\n",
    "        super().__init__()\n",
    "        weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
    "        self.model = resnet50(weights=weights)\n",
    "        self.model.eval()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Register hooks once (optional) or you can call layers directly (see below)\n",
    "        self.features = []\n",
    "        self.handles = []\n",
    "        self.handles.append(self.model.layer2[-1].register_forward_hook(self._hook))\n",
    "        self.handles.append(self.model.layer3[-1].register_forward_hook(self._hook))\n",
    "\n",
    "        if device is not None:\n",
    "            self.to(device)\n",
    "\n",
    "    def _hook(self, module, input, output):\n",
    "        # keep on-device detached copy\n",
    "        self.features.append(output.detach())\n",
    "\n",
    "    def forward(self, x, target_spatial=None):\n",
    "        # clear\n",
    "        self.features = []\n",
    "\n",
    "        # run backbone (no grad)\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(x)\n",
    "\n",
    "\n",
    "        # target spatial default = layer2 size\n",
    "        if target_spatial is None:\n",
    "            h = self.features[0].shape[-2]\n",
    "            w = self.features[0].shape[-1]\n",
    "            target_spatial = (h, w)\n",
    "\n",
    "        resized = []\n",
    "        for fmap in self.features:\n",
    "            # smooth (keep same size)\n",
    "            fmap_smoothed = F.avg_pool2d(fmap, kernel_size=3, stride=1, padding=1)\n",
    "            # resize to target spatial using interpolate (works on MPS and CPU)\n",
    "            if fmap_smoothed.shape[-2:] != target_spatial:\n",
    "                fmap_resized = F.interpolate(fmap_smoothed, size=target_spatial,\n",
    "                                             mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                fmap_resized = fmap_smoothed\n",
    "            resized.append(fmap_resized)\n",
    "\n",
    "        patch = torch.cat(resized, dim=1)\n",
    "        return patch\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for h in self.handles:\n",
    "            h.remove()\n",
    "        self.handles = []\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb7044-c651-4877-b827-ec3d8d94a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Using existing 'extractor' in memory.\n",
      "Found categories: ['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
      "=== Evaluating category: bottle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3079c8b8c61c4beb94ea503ee33649e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bottle train->scores:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle: train_scores mean=0.056990 std=0.016726 threshold=0.107169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48de33d99c764f959f8e08104465c865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bottle test:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle  AUROC(image)=1.0  Acc=1.0000  ConfMat:\n",
      "[[20  0]\n",
      " [ 0 63]]\n",
      "=== Evaluating category: cable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f2ffcc97d34cfebd3b656af91c48bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cable train->scores:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cable: train_scores mean=0.108250 std=0.012219 threshold=0.144907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f4786ebbde4e23a7acc06f60166c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cable test:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cable  AUROC(image)=0.9576461769115443  Acc=0.8800  ConfMat:\n",
      "[[56  2]\n",
      " [16 76]]\n",
      "=== Evaluating category: capsule\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2be4b741ed4b91bcf4aaa8445271f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "capsule train->scores:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capsule: train_scores mean=0.053120 std=0.011687 threshold=0.088181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ee83dc1c33492484823e1d6d92e488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "capsule test:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capsule  AUROC(image)=0.934982050259274  Acc=0.7424  ConfMat:\n",
      "[[22  1]\n",
      " [33 76]]\n",
      "=== Evaluating category: carpet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd713b214dfb4806bde89d06dee03ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "carpet train->scores:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carpet: train_scores mean=0.036568 std=0.007316 threshold=0.058516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1794551a14e9a8d672347a03af0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "carpet test:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carpet  AUROC(image)=0.9742063492063492  Acc=0.9407  ConfMat:\n",
      "[[26  2]\n",
      " [ 5 85]]\n",
      "=== Evaluating category: grid\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a622160b8394cbe8b172a7dcf582692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "grid train->scores:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid: train_scores mean=0.069295 std=0.007374 threshold=0.091418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b126ea110d49a4b3d55c4c12e1ef04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "grid test:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid  AUROC(image)=0.9741019214703426  Acc=0.9487  ConfMat:\n",
      "[[21  0]\n",
      " [ 4 53]]\n",
      "=== Evaluating category: hazelnut\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0743a036844af99f768d92ac7f6372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hazelnut train->scores:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hazelnut: train_scores mean=0.092479 std=0.013450 threshold=0.132829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d23bccef54993abd74fb08afa3475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hazelnut test:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hazelnut  AUROC(image)=1.0  Acc=1.0000  ConfMat:\n",
      "[[40  0]\n",
      " [ 0 70]]\n",
      "=== Evaluating category: leather\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e3c3369749428482512d7e2f061307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "leather train->scores:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leather: train_scores mean=0.034241 std=0.005330 threshold=0.050232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deae00f1165641d5a604c9aff2f338e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "leather test:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leather  AUROC(image)=1.0  Acc=0.9758  ConfMat:\n",
      "[[29  3]\n",
      " [ 0 92]]\n",
      "=== Evaluating category: metal_nut\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015606246b1b4c7aac61ffbde2bf82f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "metal_nut train->scores:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metal_nut: train_scores mean=0.081729 std=0.010479 threshold=0.113166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf6ea32f1cf41a790edf904e835e0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "metal_nut test:   0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metal_nut  AUROC(image)=1.0  Acc=1.0000  ConfMat:\n",
      "[[22  0]\n",
      " [ 0 93]]\n",
      "=== Evaluating category: pill\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133566343c154e449738f57ce8cfd7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pill train->scores:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pill: train_scores mean=0.060661 std=0.010564 threshold=0.092354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c4685783c046668f490fd0603dac92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pill test:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pill  AUROC(image)=0.9623567921440263  Acc=0.7964  ConfMat:\n",
      "[[ 26   0]\n",
      " [ 34 107]]\n",
      "=== Evaluating category: screw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500f800516b24c8a980f93ff0c0429fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "screw train->scores:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screw: train_scores mean=0.076264 std=0.010615 threshold=0.108109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dbacbae7e04344b14489235f14eb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "screw test:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screw  AUROC(image)=0.8466898954703833  Acc=0.6250  ConfMat:\n",
      "[[40  1]\n",
      " [59 60]]\n",
      "=== Evaluating category: tile\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229ad7b5e4f242a4b4d64db3697e08b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tile train->scores:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile: train_scores mean=0.051160 std=0.013139 threshold=0.090578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94c3db242d0479fb5b37d26ee363664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tile test:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile  AUROC(image)=1.0  Acc=1.0000  ConfMat:\n",
      "[[33  0]\n",
      " [ 0 84]]\n",
      "=== Evaluating category: toothbrush\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba0c1c3470a43f293a8b134d466399d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "toothbrush train->scores:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toothbrush: train_scores mean=0.097803 std=0.010181 threshold=0.128346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9a126d09e6431588367e6900db784c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "toothbrush test:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toothbrush  AUROC(image)=0.9361111111111111  Acc=0.8333  ConfMat:\n",
      "[[ 9  3]\n",
      " [ 4 26]]\n",
      "=== Evaluating category: transistor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b85cbcbfd3454cb83823e89fe2a822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transistor train->scores:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transistor: train_scores mean=0.089458 std=0.011488 threshold=0.123923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656f5f8c24144aab8f8a9a406b9954a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transistor test:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transistor  AUROC(image)=0.96875  Acc=0.9400  ConfMat:\n",
      "[[59  1]\n",
      " [ 5 35]]\n",
      "=== Evaluating category: wood\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a2f656c62646639dad092d6b9c5272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wood train->scores:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wood: train_scores mean=0.050232 std=0.015318 threshold=0.096185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e494d05d48944aef805c49683c23fe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wood test:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wood  AUROC(image)=0.9859649122807018  Acc=0.9494  ConfMat:\n",
      "[[16  3]\n",
      " [ 1 59]]\n",
      "=== Evaluating category: zipper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afe52e0b3ea458cb68471369edf90e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zipper train->scores:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipper: train_scores mean=0.038016 std=0.009355 threshold=0.066080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d08ab326adf4da29d5b8304d451f195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zipper test:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipper  AUROC(image)=0.9692752100840336  Acc=0.9404  ConfMat:\n",
      "[[ 28   4]\n",
      " [  5 114]]\n",
      "Saved per-image results to cae_image_results.csv\n",
      "Saved per-category summary to cae_per_category_summary.csv\n",
      "\n",
      "Summary per-category:\n",
      "      category  train_mean  train_std  threshold  image_AUROC  accuracy  \\\n",
      "0       bottle    0.056990   0.016726   0.107169     1.000000  1.000000   \n",
      "1        cable    0.108250   0.012219   0.144907     0.957646  0.880000   \n",
      "2      capsule    0.053120   0.011687   0.088181     0.934982  0.742424   \n",
      "3       carpet    0.036568   0.007316   0.058516     0.974206  0.940678   \n",
      "4         grid    0.069295   0.007374   0.091418     0.974102  0.948718   \n",
      "5     hazelnut    0.092479   0.013450   0.132829     1.000000  1.000000   \n",
      "6      leather    0.034241   0.005330   0.050232     1.000000  0.975806   \n",
      "7    metal_nut    0.081729   0.010479   0.113166     1.000000  1.000000   \n",
      "8         pill    0.060661   0.010564   0.092354     0.962357  0.796407   \n",
      "9        screw    0.076264   0.010615   0.108109     0.846690  0.625000   \n",
      "10        tile    0.051160   0.013139   0.090578     1.000000  1.000000   \n",
      "11  toothbrush    0.097803   0.010181   0.128346     0.936111  0.833333   \n",
      "12  transistor    0.089458   0.011488   0.123923     0.968750  0.940000   \n",
      "13        wood    0.050232   0.015318   0.096185     0.985965  0.949367   \n",
      "14      zipper    0.038016   0.009355   0.066080     0.969275  0.940397   \n",
      "\n",
      "    n_test  \n",
      "0       83  \n",
      "1      150  \n",
      "2      132  \n",
      "3      118  \n",
      "4       78  \n",
      "5      110  \n",
      "6      124  \n",
      "7      115  \n",
      "8      167  \n",
      "9      160  \n",
      "10     117  \n",
      "11      42  \n",
      "12     100  \n",
      "13      79  \n",
      "14     151  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ----- USER EDITABLE / CHECK these paths -----\n",
    "MVTEC_ROOT = Path(\"/Users/mrinalseth13331/Downloads/archive\")                     # root of original MVTec dataset\n",
    "CAE_DIR = Path(\"saved_spatial_caes_simple\")    # where CAE checkpoints are stored, named cae_<cat>.pth\n",
    "RESULTS_CSV = Path(\"cae_image_results.csv\")    # per-image results saved here\n",
    "PERF_CSV = Path(\"cae_per_category_summary.csv\")# per-category summary\n",
    "# ----------------------------------------------\n",
    "\n",
    "# device & transform (must match training preprocessing)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "IMAGENET_MEAN = [0.485,0.456,0.406]\n",
    "IMAGENET_STD  = [0.229,0.224,0.225]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# helper: ensure your extractor exists; if not, create it (Option A extractor)\n",
    "# If you already have `extractor` in the notebook, this will skip creating a new one.\n",
    "try:\n",
    "    extractor  # noqa: F821\n",
    "    print(\"Using existing 'extractor' in memory.\")\n",
    "except NameError:\n",
    "    print(\"No 'extractor' found in memory â€” creating a fresh ResNetFeatureExtractorFast.\")\n",
    "    # Paste the Option A class code here if not defined already in your notebook.\n",
    "    # For brevity, assume you have `ResNetFeatureExtractorFast` class defined above. If not,\n",
    "    # please paste the class cell (Option A) before running this evaluation cell.\n",
    "    extractor = ResNetFeatureExtractorFast(pretrained=True, device=device)\n",
    "    extractor.model.eval()\n",
    "    for p in extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# check FeatCAE is defined\n",
    "if 'FeatCAE' not in globals():\n",
    "    raise RuntimeError(\"FeatCAE class not found in notebook. Define FeatCAE (same as used when training) before running evaluation.\")\n",
    "\n",
    "# build category list from MVTec root (folders inside mvtec/)\n",
    "category_list = sorted([p.name for p in MVTEC_ROOT.iterdir() if p.is_dir()])\n",
    "print(\"Found categories:\", category_list)\n",
    "\n",
    "# helper: data loader for training 'good' images of a given category\n",
    "def loader_train_good_for_category(cat_name, bs=16):\n",
    "    good_dir = MVTEC_ROOT / cat_name / \"train\" / \"good\"\n",
    "    if not good_dir.exists():\n",
    "        # fallback to using mvtec_all/train/<cat> if you prepared it\n",
    "        alt_dir = Path(\"mvtec_all\") / \"train\" / cat_name\n",
    "        if alt_dir.exists():\n",
    "            ds = ImageFolder(str(alt_dir.parent), transform=transform)  # careful: we'll filter; but simpler below\n",
    "            # Build subset as earlier; but easier: let us build a simple list loader:\n",
    "            imgs = sorted(list(alt_dir.glob(\"*.png\")) + list(alt_dir.glob(\"*.jpg\")))\n",
    "            def gen():\n",
    "                for p in imgs:\n",
    "                    yield transform(Image.open(p).convert(\"RGB\"))\n",
    "            # create a DataLoader from tensor dataset after stacking might be easier, but simpler to just raise\n",
    "            raise RuntimeError(f\"No 'train/good' at {good_dir} and alternate {alt_dir} handling not implemented. Create folders or change path.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"No training-good folder for category {cat_name}: expected {good_dir}\")\n",
    "    # use a simple dataset wrapper\n",
    "    class SimpleFolderDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, files, transform):\n",
    "            self.files = files\n",
    "            self.transform = transform\n",
    "        def __len__(self): return len(self.files)\n",
    "        def __getitem__(self, idx):\n",
    "            p = self.files[idx]\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            return self.transform(img), 0\n",
    "    files = sorted(list(good_dir.glob(\"*.png\")) + list(good_dir.glob(\"*.jpg\")))\n",
    "    ds = SimpleFolderDataset(files, transform)\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=False, num_workers=0), files\n",
    "\n",
    "# helper: gather test images recursively under mvtec/<cat>/test\n",
    "def gather_test_images(cat_name):\n",
    "    test_root = MVTEC_ROOT / cat_name / \"test\"\n",
    "    if not test_root.exists():\n",
    "        raise RuntimeError(f\"No test folder for {cat_name} at {test_root}\")\n",
    "    img_paths = sorted(list(test_root.rglob(\"*.png\")) + list(test_root.rglob(\"*.jpg\")))\n",
    "    # filter out any possible ground_truth masks\n",
    "    img_paths = [p for p in img_paths if \"ground_truth\" not in p.parts]\n",
    "    return img_paths\n",
    "\n",
    "# vectorized \"decision_function\" that takes a tensor seg_map of shape (B,1,H,W)\n",
    "def topk_mean_score_from_segmap(seg_map, topk=10):\n",
    "    \"\"\"\n",
    "    seg_map: tensor (B,1,H,W) or (B,H,W)\n",
    "    returns: tensor of shape (B,) of top-k mean values (float)\n",
    "    \"\"\"\n",
    "    if seg_map.dim() == 4:\n",
    "        B = seg_map.shape[0]\n",
    "        flat = seg_map.view(B, -1)\n",
    "    elif seg_map.dim() == 3:\n",
    "        B = seg_map.shape[0]\n",
    "        flat = seg_map.view(B, -1)\n",
    "    else:\n",
    "        raise ValueError(\"seg_map must be 3D or 4D tensor\")\n",
    "    # sort descending and take topk\n",
    "    if flat.shape[1] < topk:\n",
    "        topk = flat.shape[1]\n",
    "    # use partition for speed\n",
    "    kth = torch.topk(flat, k=topk, dim=1, largest=True, sorted=False)[0]  # (B, topk)\n",
    "    topk_mean = kth.mean(dim=1)\n",
    "    return topk_mean  # shape (B,)\n",
    "\n",
    "# compute heatmap upsample and scores (returns heat as numpy HxW, and score float)\n",
    "def compute_heat_and_topk_score(img_tensor, extractor, cae, up_size=(224,224), topk=10):\n",
    "    with torch.no_grad():\n",
    "        patch = extractor(img_tensor)                      # (1,1536,h,w)\n",
    "        recon = cae(patch)                                 # (1,1536,h,w)\n",
    "        err = ((patch - recon) ** 2).mean(dim=1, keepdim=True)  # (1,1,h,w)\n",
    "        # crop border if CAE training used cropping (you used [3:-3,3:-3] earlier). We'll keep full map then optionally crop.\n",
    "        # upsample to image size\n",
    "        up = F.interpolate(err, size=up_size, mode='bilinear', align_corners=False)  # (1,1,224,224)\n",
    "        up_np = up.squeeze().cpu().numpy()\n",
    "        score = topk_mean_score_from_segmap(up, topk=topk).item()\n",
    "    heat = (up_np - up_np.min()) / (up_np.max() - up_np.min() + 1e-8)\n",
    "    return heat, score, up_np\n",
    "\n",
    "# storage for per-image rows\n",
    "rows = []\n",
    "\n",
    "# per-category summary results\n",
    "summary_rows = []\n",
    "\n",
    "# Main loop: for each category, load CAE, compute threshold from train/good, then evaluate test images\n",
    "for cat in category_list:\n",
    "    ckpt = CAE_DIR / f\"cae_{cat}.pth\"\n",
    "    if not ckpt.exists():\n",
    "        print(f\"CAE for {cat} not found at {ckpt}, skipping category.\")\n",
    "        continue\n",
    "\n",
    "    print(\"=== Evaluating category:\", cat)\n",
    "    # instantiate CAE and load weights\n",
    "    cae = FeatCAE(in_channels=1536, latent_dim=LATENT_DIM, is_bn=True)  # ensure LATENT_DIM matches your training\n",
    "    cae.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "    cae.to(device).eval()\n",
    "\n",
    "    # compute train normal scores (to get threshold)\n",
    "    train_loader_cat, train_files = loader_train_good_for_category(cat, bs=8)\n",
    "    recon_scores = []\n",
    "    for imgs, _ in tqdm(train_loader_cat, desc=f\"{cat} train->scores\"):\n",
    "        imgs = imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            patch = extractor(imgs)   # (B,1536,h,w)\n",
    "            recon = cae(patch)\n",
    "            err = ((patch - recon) ** 2).mean(dim=1, keepdim=True)  # (B,1,h,w)\n",
    "            up = F.interpolate(err, size=(224,224), mode='bilinear', align_corners=False)\n",
    "            # compute top-10 mean per image using vectorized function\n",
    "            topk_mean = topk_mean_score_from_segmap(up, topk=10)  # (B,)\n",
    "            recon_scores.extend(topk_mean.cpu().numpy().tolist())\n",
    "\n",
    "    recon_scores = np.array(recon_scores)\n",
    "    if recon_scores.size == 0:\n",
    "        print(f\"No training scores computed for {cat} (empty train). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # compute threshold: mean + 3*std\n",
    "    best_threshold = float(recon_scores.mean() + 3*recon_scores.std())\n",
    "    print(f\"{cat}: train_scores mean={recon_scores.mean():.6f} std={recon_scores.std():.6f} threshold={best_threshold:.6f}\")\n",
    "\n",
    "    # Evaluate on test images\n",
    "    img_paths = gather_test_images(cat)\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    y_pred = []\n",
    "\n",
    "    for p in tqdm(img_paths, desc=f\"{cat} test\"):\n",
    "        # determine label from folder name: parent folder 'good' => normal else anomalous\n",
    "        label_folder = p.parent.name.lower()\n",
    "        is_normal = (\"good\" in label_folder)\n",
    "        y_true_label = 0 if is_normal else 1\n",
    "\n",
    "        # load image and compute score\n",
    "        img_tensor = transform(Image.open(p).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        heat, score, raw = compute_heat_and_topk_score(img_tensor, extractor, cae, up_size=(224,224), topk=10)\n",
    "\n",
    "        y_true.append(y_true_label)\n",
    "        y_score.append(score)\n",
    "        y_pred.append(1 if score >= best_threshold else 0)\n",
    "\n",
    "        rows.append({\n",
    "            \"category\": cat,\n",
    "            \"image\": str(p),\n",
    "            \"label\": y_true_label,\n",
    "            \"score\": score,\n",
    "            \"pred\": 1 if score >= best_threshold else 0\n",
    "        })\n",
    "\n",
    "    # compute metrics for this category\n",
    "    try:\n",
    "        img_auroc = roc_auc_score(y_true, y_score) if len(set(y_true))>1 else None\n",
    "    except Exception as e:\n",
    "        img_auroc = None\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"{cat}  AUROC(image)={img_auroc}  Acc={acc:.4f}  ConfMat:\\n{cm}\")\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"category\": cat,\n",
    "        \"train_mean\": float(recon_scores.mean()),\n",
    "        \"train_std\": float(recon_scores.std()),\n",
    "        \"threshold\": best_threshold,\n",
    "        \"image_AUROC\": float(img_auroc) if img_auroc is not None else None,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"n_test\": len(y_true)\n",
    "    })\n",
    "\n",
    "# save results\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "pd.DataFrame(summary_rows).to_csv(PERF_CSV, index=False)\n",
    "print(\"Saved per-image results to\", RESULTS_CSV)\n",
    "print(\"Saved per-category summary to\", PERF_CSV)\n",
    "\n",
    "print(\"\\nSummary per-category:\")\n",
    "print(pd.DataFrame(summary_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4061e-5876-4faa-ab66-ceca87305840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
